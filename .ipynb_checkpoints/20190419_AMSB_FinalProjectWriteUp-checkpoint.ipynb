{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LLCU612 Final Project (Winter 2019)</h1>\n",
    "<h2>Pandas and Seaborn for Sentiment and Topic Analysis of Twitter Data</h2>\n",
    "<p>By Sunyam Bagga and Alayne Moody</p>\n",
    "<p>April 19, 2019</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our team is comprised of a computer scientist, Sunyam Bagga, and a digital humanist, Alayne Moody. We have disparate but complementary skills and research interests. Our collaboration developed with ease, beginning with a brief word in class, followed by a Spring Break email exchange about a project idea and the quick recognition that we had enough common ground to form a team. We each came to the table with particular abilities, resources and ideas. In addition to management and writing skills, Alayne provided:<ul> \n",
    "    <li>the dataset (a custom collection of tweets), </li>\n",
    "    <li>a journal article to serve as a starting point (Ozturk & Ayvaz, 2018), and </li>\n",
    "    <li>a workflow and toolkit proposal - weekly meetings, with Asana for communication, Dropbox for data and GitHub for code. </li></ul>\n",
    "With his programming skills and experience working with text-based data projects, Sunyam was well positioned to set our technical skill building objectives, which were to develop our capacities with:<ul>\n",
    "    <li>Pandas for manipulating, processing and analyzing data,</li>\n",
    "    <li>Matplotlib and Seaborn for data visualizations,</li>\n",
    "    <li>VADER for calculating sentiment scores, and</li>\n",
    "    <li>Gensim for modelling topics.</li></ul>\n",
    "As the project developed, our third team member, Stefan, contributed ideas, such as adding an automatic language detector and a second form of sentiment scoring.</p>\n",
    "<p>Our objective was to partially replicate and extend the work of Nazan Ozturk and Serkan Ayvaz in their 2018 <i>Telematics and Informatics</i> paper \"Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis.\" Our dataset is an original collection of 123,524 tweets harvested between January 21 and March 4, 2019, through the Twitter Archiving Google Sheet (TAGS), which interfaces with the Twitter Search API. Our research question was: How do the tweets in our collection vary, particularly in terms of time, sentiment, topic and author location.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Starting Point & Digressions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ozturk and Ayvaz (2018) compare sentiment in 2,381,197 English and Turkish language tweets collected via the Twitter Search API between March 29 and April 30, 2017. Search criteria included the keywords \"Syrian,\" \"refugee\" and three corresponding Turkish words. Although Turkey was accepting more Syrian refugees than the United States, the sentiment of Turkish tweets was more positive than the sentiment of U.S. tweets. Most notably, 35% of Turkish tweets carried positive sentiment compared to only 12% for U.S. tweets. We agreed this article could serve as a good starting point for our project because it offered compelling subject matter (migration), methods that were relevant and offered the right balance of challenge and feasibility, and opportunities for improvement, especially in terms of the topic analysis, data visualization and interpretation of results.</p>\n",
    "<p>For our partial replication, we focus on English language tweets for multiple reasons, the most important being the distribution of languages in our dataset. The authors of 106,942 tweets indicated English as their primary language, followed by 4,523 for French, 3,016 for German and 2,488 for Spanish. The representation of other languages declined steeply from there. The United States also heavily dominated the author location variable. With these distributions in mind, we decided to focus on English language tweets and compare sentiment by author location. This approach also addressed a shortcoming we perceived in the original study: the lack of a baseline showing whether English language tweets were generally negative. While our modification would not allow us to address this exact issue, it would shed light on whether English tweets coming from the United States were more negative than English tweets from other parts of the world.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We used Pandas to clean and process the numerical and categorical data and NLTK to process the unstructured textual data (that is, the text of the tweets). Our original dataset included the following information:<ul> \n",
    "    <li><b>tweet level</b>. ID, time, date, components (e.g., hashtags, user mentions, links), URL, method (e.g., web app, phone, Facebook), geographical coordinates</li>\n",
    "    <li><b>author level.</b> ID, screen name, language, location, follower count, friend count, picture URL</li>\n",
    "    <li><b>retweet level.</b> ID of original tweet, ID and screen name of original author</li></ul>\n",
    "Many of these variables contained missing data or required recoding of values. In particular, the author location and the time-date variables required considerable wrangling. To reclassify the large number of nonstandardized location values, we created dictionaries associating U.S. state names and abbreviations to the value \"USA\" and mapped original values to new values using a for-loop. This left a large number of U.S. locations that needed to be manually reassigned to the \"USA\" category using regular expression. The time-date variable needed to be transformed into the timestamp data type and stripped of the time information to facilitate the time series analysis. The scripts used for the above work are available at:\n",
    "<ul><li><a href=\"https://github.com/menyalas/llcu612Project/blob/master/Descriptives-SAM.ipynb\">Descriptives-SAM.ipynb</a> - exploration, cleaning & recoding of numerical & categorical data</li>\n",
    "    <li><a href=\"http://localhost:8889/notebooks/llcu612Project/llcu612Project/Tweets-Analysis.ipynb\">Tweets-Analysis.ipynb</a> - exploration, cleaning and analysis of tweet text</li>\n",
    "    <li><a href=\"http://localhost:8889/notebooks/llcu612Project/llcu612Project/TimeLocation.ipynb\">TimeLocation.ipynb </a> - transformation and recording of the time and location variables</li>\n",
    "    <li><a href=\"http://localhost:8889/notebooks/llcu612Project/llcu612Project/20190406_AM_Dataset.ipynb\">20190406_AM_Dataset.ipynb</a> - consolidation of data preprocessing steps and saving of final dataset</li></ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLP Variables</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>In addition to preparing existing variables for analysis, we created several new variables measuring latent features. We calculated sentiment scores using the AFINN and VADER lexicons because both performed well in previous studies (Islam and Zibram, n.d.). Topics were modelled using Gensim and Latent Dirichlet Allocation (LDA), with five being the number of topics we eventually settled on. The word vectors for our topics are:<ol>\n",
    "    <li><b>Humanitarian.</b>Group, get, watch, american, new, refugee, crisis, people, right, guess</li>\n",
    "    <li><b>Politics.</b> Caravan, know, trump, family, u, prevent, implement, vote, need, today</li>\n",
    "    <li><b>Conflict.</b> Europe, pact, hungary, voice, brussels, v_of_europe, illegal, life, police, street</li>\n",
    "    <li><b>Minors.</b> Child, gang, free, part, say, government, u.s., youth, worker, salvadoran</li>\n",
    "    <li><b>Security.</b> Border, law, cross, arizona, via, bus, section, unsecured, unloads, stop</li></ol>\n",
    "The language of the tweets, as opposed to the language of the authors as indicated in their profiles, was determined using the langdetect module, revealing 9,680 instances when the language of the tweet does not match the language of the author. We also created an indicator variable for whether the tweet was a retweet or not.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary Statistics & Visualizations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All variables in the original dataset were examined using descriptive statistical methods, but only the variables of primary interest are reported here. These are author location, sentiment, topic, language and tweet method. Please see <a href=\"Descriptives-SAM.ipynb\">Descriptives-SAM.ipynb</a> to see the results for the other variables; however, please note that this notebook was prepared before we decided to focus on English language tweets. The results shown there are therefore for the entire dataset.</p>\n",
    "<p>The subset used for analysis includes 111,785 English language tweets, of which 106,003 (95%) had self-identified anglophone authors and 5,782 (5%) had non-anglophone authors. Seventy percent of the tweets had authors who identitied their location, with 41,283 tweets coming from authors living in the United States and and 36,490 tweets coming from authors living elsewhere. </p>\n",
    "<p>VADER sentiment scores ranged from -0.99 to 0.98 with a mean of -0.18. The distribution shows heavy clustering at zero with greater density in the negative realm, a pattern that held up for both author group categoies, although it was stronger in the United States, as shown in Figure 1. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the sentiment scores were recoded into categorical data, the \"negative\" tweets had the highest count (43,949), followed by \"positive\" (26,000), \"neutral\" (20,199), \"Very Negative\" (15,895) and \"Very Positive\" (5,742). A breakdown of these counts by author location is shown in Figure 2.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sixty percent of the tweets (67,075) were issued via smartphone, with 20% (21,899) via the standard web app, 14% (15,576) via the tablet and iPad friendly lite web app and 6% (7,235) via 3rd party interfaces, such as Facebook and LinkedIn. Figure three shows the distribution of sentiment scores for each tweet method broken down by coutry. Note that in the United States, the Twitter utilities designed for more mobile devices are associated with more negative sentiment tweets.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet was associated with a single topic, with 76% of the tweets showing humanitarian (46,527) or political (37,876) concerns. The remaining 25% of the tweets were concerned with conflict (10,985), minors (9,930) or security (6,467). Compared with the rest of the world, U.S. authors are less concerned about humanitarian issues and more concerned about politics, a finding that is consistent with Ozturk and Ayvaz (2018), who report a similar pattern in U.S. and Turkish tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Also consistent with the findings of Ozturk and Ayvaz (2018), U.S. authors in our study differentiated themselves less when the conversation involved children. Figure 5 shows this finding as well as the added similarity between the sentiment of U.S. and non-U.S. authors on matters of security. On the two preeminent topics of politics and humanitarian affairs, U.S. author are more like the rest of the world on political matters than on humanitarian issues, where their sentiment scores are considerably lower. In general, U.S. authors are more negative except on the issue of security where they have a slightly more positive average tone than the rest of the world.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The number of tweets per day in our dataset range from 2,095 to 2,841 with a mean of 2,599. The day with the most tweets was January 26 and the day with the fewest tweets was February 26. Figure 6 shows the sentiment scores by date for U.S. and non U.S. authors. The most negative scores occured around the days with the two greatest number of tweets. The most positive scores occured on February 3, when there were 2,640 tweets, placing it in the 3rd quartile of the distribution for daily tweet count.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Current Events Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Like Ozturk and Ayvaz (2018), we attempt to place the twitter traffic and sentiment spikes in our dataset within a current events context. An important story developing during the tweet collection period was the attempt by U.S. President Donald Trump to direct federal funding to the construction of a southern border wall. His demands led to a partial shutdown of the U.S. Government from December 22 through January 25. His failure achieve his goal through the Congressional budgetary process was followed by his declaration on February 15 of a national emergency, which he planned to use as justification for funding the wall. With these events in mind, we consulted the New York Times headline archive on the days with the two most negative tweets for clues about might be driving Tweet activity:<ul>\n",
    "<li>January 26 - Trump Signs Bill Reopening Government for 3 Weeks in Surprise Retreat From Wall</li>\n",
    "    <li>January 26 - Mexico Protests U.S. Decision to Return Asylum Seekers</li>\n",
    "    <li>February 26 - G.O.P. Tries to Hold Down Defections Before Vote to Block Trump’s Emergency</li>\n",
    "    <li>February 26 - Jeremy Corbyn, Under Pressure From His Labour Party, Backs New Brexit Vote</li></ul>\n",
    "<p>These headlines reflect the southern border wall controversy as well as a European  story that might have been influencing Twitter activity. The story is similarly politically oriented, referring to the move by opposition party leader Jeremy Corbyn to allow a second public referendum on question of the U.K. planned departure from the European Union. The wave of nationalist populism that contributed to Donald Trump's victory in the U.S. presidential elections is associated with the Brexit movement, with both having a distinct anti-migrant flavor. </p>\n",
    "<p>Indentifying the stories that might have driven the positive sentiment spike on migrant and migration related tweets on February 3 is more difficult to do, given that headlines tend to capture controversial or otherwise \"bad\" news. The day with our most positive twitter content is also a Sunday, a notoriously quiet news day. Indeed, the New York Times headline archive shows no indication what might have driven sentiment so high. We therefore checked the headlines for the following day to see if events occuring on Sunday might appear in Mondays headlines, with the understanding that news spreads in far more ways than just via the New York Times. Indeed, the news headlines in the New York Times on Monday February 4th include:<ul>\n",
    "    <li>Australia Says Last Refugee Children Held on Nauru Will Go to U.S.</li>\n",
    "    <li>ICE Takes the Rapper 21 Savage Into Custody, Officials Say</li></ul></p>\n",
    "<p>We looked for evidence of these stories in our dataset by generating word clouds of the tweets for each of the key dates of interest. The two negative word clouds are in Figures 7 (January 26, 2019) and 8 (February 26, 2019).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_07.png)\n",
    "![title](20180418_FinalReport_08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All clouds include the 100 most frequent terms used in tweets on the key dates. The word cloud for January 26 includes terms (e.g., gopchairwoman, realdonaldtrump, voting, democrats, border, wall, security) that reflect the politically charged border wall story but the not the story about the return of asylum seekers to Mexico. The February 26 word cloud does not align at all with the stories headling in the New York times on February 26. Rather than words related to an impending vote on the emergency border wall funding or to the issue of a second Brexit vote, we see terms that seem associated with the \"Humanitarian\" or \"Minors\" topic, with several prominent terms referring to sexuality and abuse. Further examination of news coverage for the day show National Public Radio, BBC, CNN and other organizations reporting thousands of immigrant children being sexually abused while in U.S. detention centers. The New York Times eventually reported this story on February 27.</p>\n",
    "<p>The word cloud for the positive sentiment spike observed on Sunday, February 3, includes tweets on that day as well as Monday, February 4. The 100 most frequent terms are shown in Figure 9.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20180418_FinalReport_09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>While we see some terms that might derive from the news story about refugee minors form Nauru being relocated to the United States, there appears to be other factors at play. The term \"great\" is prominent. Part of President Trump's \"Make American Great\" campaign slogan, it is a term that likely scores highly on positive sentiment, but within the context of migrants and migration has a distinctly negative connotation. Likewise, the term \"build\" is important in this word cloud, and certainly has positive sentiment in a general sense; howevere, its co-occurrence with the term \"wall\" (as in along the southern border of the United States to keep migrants out) means that it should probably be scored negatively in the context of our study. Some less prominent terms (e.g., \"parents\", \"children\") might relate to the story of migrant families who had been separated by U.S. border officials and were in the process of being reunited.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For our final project, we analyzed an original collection of migration related tweets from the period January 21 to March 4, 2019. Inspired by a descriptive study of sentiment in a similar tweetset and by a desire to develop our Python skills in the areas of text analysis, dataframe manipulation, statistics and data visualization, we decided to team up to do a partial replication and extension of the original study. Looking only at English language tweets and comparing sentiment in tweets from authors based in and out of the United States, we aimed to answer the following research question: How do the tweets in our collection vary, particularly in terms of time, sentiment, topic and author location? Using NLTK, Pandas, VADER, Gensim, Seaborn and Wordcloud, among other Python libraries and modules, we processed, analyzed and visualizaed observed and latent data more than 100,000 tweets containing the term \"migrant\" or \"migration.\" The main finding is that English langauge tweets about migrants and migration authored by U.S. residents were more negative than those authored by non U.S. residents, except when the tweet had to do with children. U.S authors focused more on politics than humanitarian affairs than their counterparts outside the United States. More mobile devices appeared to be associated with more negative tweets in the United States. Observed spikes in sentiment analsysis on certain dates could only be partially explained through the use of word clouds and news coverage of current events.</p>\n",
    "<p>Our findings are limited in that the Twitter Search API does not generate a representative sample. For example, González-Bailón et al. (2014) have shown that samples drawn from the search API favor underrepresent peripheral activity (González-Bailón et al., 2014). This appears to result in biases in the dataset toward English language tweets and tweets coming from the United States. In order to make inference to a population beyond our sample, we would need to acquire a probabilistic sample or be able to account with some certainty for the biases in our sample. This next step would enable us to use inferential methods that extend beyond our descriptive presented here.</p>\n",
    "<p>Another extension of the work presented here would be to look more closely at network variables, such as friend and follower counts, mentions, hashtags and the original authors in retweets. It would be interesteing to explore relationships among these varables and between them and sentiment. Finally, further exploration on the langauge front would be useful, something that could be approached in many ways. Additional langauges could be added to the current analysis. Second language factors could be considered, for example, to explore how sentiment varies when authors are tweeting in and outside of the their primary language. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>González-Bailón et al. (2014). Assessing the bias in samples of large online networks. Social Networks, 38. Retrieved March 25, 2019 at https://doi.org/10.1016/j.socnet.2014.01.004</p>\n",
    "<p>Hawksey, M. (2019). Twitter archiving google sheet (TAGS). Retrieved March 25, 2019, from https://tags.hawksey.info/</p>\n",
    "<p>Islam, M. R. (n.d.) A Comparison of Dictionary Building Methods for Sentiment Analysis in Software Engineering Text. Retrieved from http://cs.uno.edu/~zibran/resources/MyPapers/DictionaryEvaluation.pdf</p>\n",
    "<p>Ozturk, N., & Ayvaz, S. (2018). Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis. <i>Telematics and Informatics</i>, 35, 136-147.</p>\n",
    "<p>New York Times. (n.d.). Todays's Paper. Retrieved from https://www.nytimes.com/issue/todayspaper/2019/02/04/todays-new-york-times</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
