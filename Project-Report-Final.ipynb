{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LLCU-612 Final Project (Winter 2019)</h1>\n",
    "<h2>Sentiment and Topic Analysis of Migrant Related Tweets</h2>\n",
    "<p>By <i>Sunyam Bagga</i> and <i>Alayne Moody</i></p>\n",
    "<p>April 19, 2019</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our team is comprised of a computer scientist, Sunyam Bagga, and a digital humanist, Alayne Moody. We have disparate but complementary skills and research interests. Our collaboration developed with ease, beginning with a brief word in class, followed by a Spring Break email exchange about a project idea and the quick recognition that we had enough common ground to form a team. We each came to the table with particular abilities, resources, and ideas. In addition to management and writing skills, Alayne provided:<ul> \n",
    "    <li>the dataset (a custom collection of tweets), </li>\n",
    "    <li>a journal article to serve as a starting point [4], and </li>\n",
    "    <li>a workflow and toolkit proposal - weekly meetings, with Asana for communication, Dropbox for data and GitHub for code. </li></ul>\n",
    "With his programming skills and experience working with text-based data projects, Sunyam was well positioned to set our technical skill building objectives, which were to develop our capacities with:<ul>\n",
    "    <li>Pandas for manipulating, processing and analyzing data,</li>\n",
    "    <li>Matplotlib and Seaborn for data visualizations,</li>\n",
    "    <li>VADER for calculating sentiment scores, and</li>\n",
    "    <li>Gensim for topic modeling.</li></ul>\n",
    "As the project developed, our third team member, Stefan, contributed ideas, such as adding an automatic language detector and a second form of sentiment scoring.</p>\n",
    "<p>Our objective was to partially replicate and extend the work of Nazan Ozturk and Serkan Ayvaz in their 2018 <i>Telematics and Informatics</i> paper \"Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis\" [4]. Our dataset is an original collection of 123,524 tweets harvested between January 21 and March 4, 2019, through the Twitter Archiving Google Sheet (TAGS) [2], which interfaces with the Twitter Search API. Our research question was: How do the tweets in our collection vary, particularly in terms of time, sentiment, topic and author location.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Starting Point & Digressions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ozturk and Ayvaz (2018) compare sentiment in 2,381,197 English and Turkish language tweets collected via the Twitter Search API between March 29 and April 30, 2017. Search criteria included the keywords \"Syrian,\" \"refugee\" and three corresponding Turkish words. With Turkey accepting more Syrian refugees than the United States, their study found the sentiment of Turkish tweets to be more positive than the sentiment of U.S. tweets. Most notably, 35% of Turkish tweets carried positive sentiment compared to only 12% for U.S. tweets. We agreed this article could serve as a good starting point for our project because it offered compelling subject matter (migration), methods that were relevant, and offered the right balance of challenge and feasibility. Moreover, there were opportunities for improvement - especially in terms of the topic analysis, data visualization, and interpretation of results.</p>\n",
    "<p>For our partial replication, we focus on English language tweets for multiple reasons, the most important being the distribution of languages in our dataset. The authors of 106,942 tweets indicated English as their primary language, followed by 4,523 for French, 3,016 for German and 2,488 for Spanish. The representation of other languages declined steeply from there. The United States also heavily dominated the author location variable. With these distributions in mind, we decided to focus on English language tweets and compare sentiment by author location. This approach also addressed a shortcoming we perceived in the original study: the lack of a baseline showing whether English language tweets were generally negative. While our modification would not allow us to address this exact issue, it would shed light on whether English tweets coming from the United States were more negative than English tweets from other parts of the world.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We used Pandas to clean and process the numerical/categorical data, and NLTK to process the unstructured textual data (that is, the text of the tweets). Our original dataset included information at the following levels:<ul> \n",
    "    <li><b>tweet level</b>: ID, time, date, components (e.g., hashtags, user mentions, links), URL, method (e.g., web app, phone, Facebook), geographical coordinates</li>\n",
    "    <li><b>author level</b>: ID, screen name, language, location, follower count, friend count, picture URL</li>\n",
    "    <li><b>retweet level</b>: ID of original tweet, ID and screen name of original author</li></ul>\n",
    "Many of these variables contained missing data or required recoding of values. In particular, the author location and the time-date variables required considerable wrangling. To reclassify the large number of nonstandardized location values, we created dictionaries associating U.S. state names and abbreviations to the value \"USA\" and mapped original values to new values using a for-loop. This left a large number of U.S. locations that needed to be manually reassigned to the \"USA\" category using regular expression. The time-date variable needed to be transformed into the timestamp data type and stripped of the time information to facilitate the time series analysis. We also created an indicator variable for whether the tweet was a retweet or not (any tweet that starts with 'RT' was a retweet).\n",
    "\n",
    "The scripts used for the above work are available at:\n",
    "<ul><li><a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/Descriptives-SAM.ipynb\">Descriptives-SAM.ipynb</a> - exploration, cleaning & recoding of numerical & categorical data</li>\n",
    "    <li><a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/Tweets-Analysis.ipynb\">Tweets-Analysis.ipynb</a> - exploration and analysis of tweet text (includes frequently used words/hashtags & language detection)</li>\n",
    "    <li><a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/TimeLocation.ipynb\">TimeLocation.ipynb </a> - transformation and recording of the time and location variables</li>\n",
    "    <li><a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/20190406_AM_Dataset.ipynb\">20190406_AM_Dataset.ipynb</a> - consolidation of data preprocessing steps and saving of final dataset</li></ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Latent Variables: Sentiment Score and Topic Modeling</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>In addition to processing existing variables for analysis, we created several new columns in our dataset measuring latent features. Firstly, we computed sentiment scores using the tweets' text. Due to absence of an annotated dataset for migrant-related tweets, we chose to use lexicon-based approaches to sentiment analysis instead of doing supervised machine learning. In particular, we used the popular VADER [6] and AFINN [3] lexicons. VADER was specifically created with a microblog-like context in mind. In [6], VADER has been shown to outperform other popular lexicons including LIWC, ANEW, SentiWordNet, and other machine learning approaches that use Naive Bayes/SVM. Therefore, the sentiment results reported in this paper correspond to VADER scores. </p>\n",
    " \n",
    " <p> Secondly, we detect the language of the tweets using Python's <a href=\"https://github.com/Mimino666/langdetect\">langdetect</a> module which is a direct port of Google's language detection library [7] from Java to Python. The language of the tweets, when compared with the language of the authors as indicated in their profiles, revealed that there were 9,680 tweets where tweet language does not match the language of the author (see <a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/Tweets-Analysis.ipynb\">Tweets-Analysis.ipynb</a>). We use this information to perform topic modeling only on English language tweets. Further analysis of this discrepancy is left for future work.</p>\n",
    " \n",
    " <p>Lastly, we perform topic modeling using Latent Dirichlet Allocation (LDA) algorithm. We use the Gensim framework [8] and pass it all our english language tweets. After experimenting with different hyperparameters, we settled on five topics. For a complete list of hyperparameters used, see our <a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/train_lda.py\">code</a> for training LDA. We get the following topic distribution as output:<ol>\n",
    "    <li><i>Humanitarian</i>: Group, get, watch, american, new, refugee, crisis, people, right, guess</li>\n",
    "    <li><i>Politics</i>: Caravan, know, trump, family, u, prevent, implement, vote, need, today</li>\n",
    "    <li><i>Conflict</i>: Europe, pact, hungary, voice, brussels, v_of_europe, illegal, life, police, street</li>\n",
    "    <li><i>Minors</i>: Child, gang, free, part, say, government, u.s., youth, worker, salvadoran</li>\n",
    "    <li><i>Security</i>: Border, law, cross, arizona, via, bus, section, unsecured, unloads, stop</li></ol>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary Statistics & Visualizations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All variables in the original dataset were examined using descriptive statistical methods, but only the variables of primary interest are reported here. These are author location, sentiment, topic, language and tweet method. Please see <a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/Descriptives-SAM.ipynb\">Descriptives-SAM.ipynb</a> to see the results for the other variables; however, please note that this notebook was prepared before we decided to focus on English language tweets. The results shown there are therefore for the entire dataset.</p>\n",
    "<p>The subset used for analysis includes 111,785 English language tweets, of which 106,003 (95%) had self-identified anglophone authors and 5,782 (5%) had non-anglophone authors. Seventy percent of the tweets had authors who identitied their location, with 41,283 tweets coming from authors living in the United States and and 36,490 tweets coming from authors living elsewhere. </p>\n",
    "<p>VADER sentiment scores ranged from -0.99 to 0.98 with a mean of -0.18. The distribution shows heavy clustering at zero with greater density in the negative realm, a pattern that held up for both author group categories, although it was stronger in the United States, as shown in Figure 1. Note that the code for generating all figures presented in this section can be found in our <a href=\"https://github.com/menyalas/llcu612Project/blob/master/code/Visualizations.ipynb\">Visualizations notebook</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 1:\n",
    "![title](images/20180418_FinalReport_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When the sentiment scores were recoded into categorical data, the \"negative\" tweets had the highest count (43,949), followed by \"positive\" (26,000), \"neutral\" (20,199), \"Very Negative\" (15,895) and \"Very Positive\" (5,742). A breakdown of these counts by author location is shown in Figure 2.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 2:\n",
    "![title](images/20180418_FinalReport_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sixty percent of the tweets (67,075) were issued via smartphone, with 20% (21,899) via the standard web app, 14% (15,576) via the tablet and iPad friendly lite web app and 6% (7,235) via 3rd party interfaces, such as Facebook and LinkedIn. Figure 3 shows the distribution of sentiment scores for each tweet method broken down by country. Note that in the United States, the Twitter utilities designed for more mobile devices are associated with more negative sentiment tweets.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 3:\n",
    "![title](images/20180418_FinalReport_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet was associated with a single topic, with 76% of the tweets showing humanitarian (46,527) or political (37,876) concerns. As can be seen in Figure 4, the remaining 25% of the tweets were concerned with conflict (10,985), minors (9,930) or security (6,467). Compared with the rest of the world, U.S. authors are less concerned about humanitarian issues and more concerned about politics, a finding that is consistent with Ozturk and Ayvaz (2018), who report a similar pattern in U.S. and Turkish tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 4:\n",
    "![title](images/20180418_FinalReport_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Also consistent with the findings of Ozturk and Ayvaz (2018), U.S. authors in our study differentiated themselves less when the conversation involved children. Figure 5 shows this finding as well as the added similarity between the sentiment of U.S. and non-U.S. authors on matters of security. On the two preeminent topics of politics and humanitarian affairs, U.S. authors are more like the rest of the world on political matters than on humanitarian issues, where their sentiment scores are considerably lower. In general, U.S. authors are more negative except on the issue of security where they have a slightly more positive average tone than the rest of the world.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 5:\n",
    "![title](images/20180418_FinalReport_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The number of tweets per day in our dataset range from 2,095 to 2,841 with a mean of 2,599. The day with the most tweets was January 26 and the day with the fewest tweets was February 26. Figure 6 shows the sentiment scores by date for U.S. and non U.S. authors. The most negative scores occured around the days with the two greatest number of tweets. The most positive scores occured on February 3, when there were 2,640 tweets, placing it in the 3rd quartile of the distribution for daily tweet count.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 6:\n",
    "![title](images/20180418_FinalReport_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Current Events Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Like Ozturk and Ayvaz (2018), we attempt to place the twitter traffic and sentiment spikes in our dataset within a current events context. An important story developing during the tweet collection period was the attempt by U.S. President Donald Trump to direct federal funding to the construction of a southern border wall. His demands led to a partial shutdown of the U.S. Government from December 22 through January 25. His failure to achieve his goal through the Congressional budgetary process was followed by his declaration on February 15 of a national emergency, which he planned to use as justification for funding the wall. With these events in mind, we consulted the New York Times headline archive on the days with the two most negative tweets for clues about what might be driving the Tweet activity:<ul>\n",
    "<li>January 26 - Trump Signs Bill Reopening Government for 3 Weeks in Surprise Retreat From Wall</li>\n",
    "    <li>January 26 - Mexico Protests U.S. Decision to Return Asylum Seekers</li>\n",
    "    <li>February 26 - G.O.P. Tries to Hold Down Defections Before Vote to Block Trump’s Emergency</li>\n",
    "    <li>February 26 - Jeremy Corbyn, Under Pressure From His Labour Party, Backs New Brexit Vote</li></ul>\n",
    "<p>These headlines reflect the southern border wall controversy as well as a European  story that might have been influencing Twitter activity. The story is similarly politically oriented, referring to the move by opposition party leader Jeremy Corbyn to allow a second public referendum on the question of the planned U.K. departure from the European Union. The wave of nationalist populism that contributed to Donald Trump's victory in the 2016 U.S. presidential election is associated with the Brexit movement, with both having a distinct anti-migrant flavor. </p>\n",
    "<p>Identifying the stories that might have driven the positive sentiment spike on migrant and migration related tweets on February 3 is more difficult to do, given that headlines tend to capture controversial or otherwise \"bad\" news. The day with our most positive twitter content is also a Sunday, a notoriously quiet news day. Indeed, the New York Times headline archive shows no indication of what might have driven sentiment so high. We therefore checked the headlines for the following day to see if events occuring on Sunday might appear in Monday's headlines, with the understanding that news spreads in far more ways than just via the New York Times. Indeed, the news headlines in the New York Times on Monday February 4th include:<ul>\n",
    "    <li>Australia Says Last Refugee Children Held on Nauru Will Go to U.S.</li>\n",
    "    <li>ICE [Immigration and Customs Enforcement] Takes the Rapper 21 Savage Into Custody, Officials Say</li></ul></p>\n",
    "<p>We looked for evidence of these stories in our dataset by generating word clouds (using the <a href=\"https://github.com/amueller/word_cloud\">word cloud python package</a>) of the tweets for each of the key dates of interest. The two negative word clouds are in Figure 7 (January 26, 2019) and Figure 8 (February 26, 2019).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 7:\n",
    "![title](images/20180418_FinalReport_07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 8:\n",
    "![title](images/20180418_FinalReport_08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All clouds include the 100 most frequent terms used in tweets on the key dates. The word cloud for January 26 includes terms (e.g., gopchairwoman, realdonaldtrump, voting, democrats, border, wall, security) that reflect the politically charged border wall story but the not the story about the return of asylum seekers to Mexico. The February 26 word cloud does not align at all with the headlines in the New York times on February 26. Rather than words related to an impending vote on the emergency border wall funding or to the issue of a second Brexit vote, we see terms that seem associated with the \"Humanitarian\" or \"Minors\" topic, with several prominent terms referring to sexuality, abuse and youths. Further examination of news coverage for the day shows National Public Radio, BBC, CNN and other organizations reporting that thousands of immigrant children claimed to have been sexually abused while in U.S. detention centers. The New York Times eventually reported this story on February 27.</p>\n",
    "<p>The word cloud for the positive sentiment spike observed on Sunday, February 3, includes tweets on that day as well as Monday, February 4. The 100 most frequent terms are shown in Figure 9.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Figure 9:\n",
    "![title](images/20180418_FinalReport_09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>While we see some terms that might derive from the news story about refugee minors form Nauru being relocated to the United States, there appears to be other factors at play. The term \"great\" is prominent. Part of President Trump's \"Make America Great\" campaign slogan, it is a term that likely scores high on positive sentiment, but within the context of migrants and migration could have a negative connotation. Likewise, the term \"build\" is important in this word cloud, and certainly has positive sentiment in a general sense; however, its co-occurrence with the term \"wall\" (as in along the southern border of the United States to keep migrants out) means that it might carry a negative sentiment in the context of our study. Some less prominent terms (e.g., \"parents\", \"children\") might relate to the story of migrant families who had been separated by U.S. border officials and were in the process of being reunited.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For our final project, we analyzed an original collection of migration related tweets from the period January 21 to March 4, 2019. Inspired by a descriptive study of sentiment in a similar dataset and by a desire to develop our Python skills in the areas of text analysis, dataframe manipulation, statistics and data visualization, we decided to team up to do a partial replication and extension of the original study. Looking only at English language tweets and comparing sentiment in tweets from authors based in and out of the United States, we aimed to answer the following research question: How do the tweets in our collection vary, particularly in terms of time, sentiment, topic and author location? </p>\n",
    "<p>Using NLTK, Pandas, VADER, Gensim, Seaborn and Wordcloud, among other Python libraries and modules, we processed, analyzed and visualized text data from more than 100,000 tweets containing the term \"migrant\" or \"migration.\" The main finding is that English langauge tweets about migrants and migration authored by U.S. residents were more negative than those authored by non U.S. residents, except when the tweet had to do with children. U.S authors focused more on politics than humanitarian affairs than their counterparts outside the United States. More mobile devices appeared to be associated with more negative tweets in the United States. Observed spikes in sentiment on certain dates could only be partially explained through the use of word clouds and analysis of news coverage of current events.</p>\n",
    "<p>Our findings are limited in that the Twitter Search API does not generate a representative sample. For example, González-Bailón et al. have shown that samples drawn from the search API underrepresent peripheral activity [1]. This appears to result in biases in the dataset toward English language tweets and tweets coming from the United States. In order to make inference about a population beyond our sample, we would need to acquire a probabilistic sample or be able to account with some certainty for the biases in our sample. This next step would enable us to use inferential methods that extend beyond the descriptive analysis presented here.</p>\n",
    "<p>Another extension of the work presented here would be to look more closely at network variables, such as friend and follower counts, mentions, hashtags and the original authors in retweets. It would be interesting to explore relationships among these varables and sentiment scores. Finally, further exploration on the langauge front would be useful, something that could be approached in many ways. Additional langauges could be added to the analysis. Second language factors could be considered, for example, to explore how sentiment varies when authors are tweeting in and outside of the their primary language. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[1] González-Bailón et al. (2014). Assessing the bias in samples of large online networks. Social Networks, 38. Retrieved March 25, 2019 at https://doi.org/10.1016/j.socnet.2014.01.004</p>\n",
    "<p>[2] Hawksey, M. (2019). Twitter archiving google sheet (TAGS). Retrieved March 25, 2019, from https://tags.hawksey.info/</p>\n",
    "<p>[3] Finn Årup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\", Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages. Volume 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)</p>\n",
    "<p>[4] Ozturk, N., & Ayvaz, S. (2018). Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis. <i>Telematics and Informatics</i>, 35, 136-147.</p>\n",
    "<p>[5] New York Times. (n.d.). Todays's Paper. Retrieved from https://www.nytimes.com/issue/todayspaper/2019/02/04/todays-new-york-times</p>\n",
    "<p>[6] Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.</p>\n",
    "<p>[7] Shuyo, Nakatani. Language Detection Library for Java (2010) http://code.google.com/p/language-detection/}</p>\n",
    "<p>[8] Radim and Petr Sojka. (2010). Software Framework for Topic Modelling with Large Corpora. Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks http://is.muni.cz/publication/884893/en</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
